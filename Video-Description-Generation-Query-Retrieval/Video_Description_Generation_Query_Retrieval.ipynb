{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b584ead1-9a08-44eb-8c03-4ab98eb0f34a",
   "metadata": {},
   "source": [
    "# Video Description Generation and Query Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50c359d-093d-49f2-8a53-43ae428355d1",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c84bdadd-ac84-4232-baf5-3394e51b6d3b",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to generate video descriptions using the [**Qwen 2.5 Vision-Language model**](https://github.com/QwenLM/Qwen2.5-VL) and store their embeddings in [**ChromaDB**](https://www.trychroma.com/) for efficient semantic search on **Intel® Core™ Ultra Processors**. The Qwen 2.5 Vision-Language model is loaded using the [**PyTorch XPU backend**](https://docs.pytorch.org/docs/stable/notes/get_start_xpu.html) to leverage Intel hardware acceleration.\\\n",
    "For each video, a description is generated and stored as an embedding in ChromaDB. When a user submits a query, cosine similarity search is performed in ChromaDB to retrieve the most relevant video description. The matching video is then displayed inline.\\\n",
    "This sample supports sports-related queries and uses a subset of videos from the [**Sports Videos in the Wild (SVW)**](https://cvlab.cse.msu.edu/project-svw.html) dataset. For more information on the dataset and citation requirements, please refer to the [**Sports Videos in the Wild (SVW) dataset paper**](https://cvlab.cse.msu.edu/project-svw.html#:~:text=SVW%20Download,Bibtex%20%7C%20PDF)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4066302a-a785-4a4d-882d-26cba0f73f6c",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087121d4-116e-4954-a84d-a8790774ec3c",
   "metadata": {},
   "source": [
    "- During the initial data load, a subset of videos from the [Sports Videos in the Wild (SVW)](https://cvlab.cse.msu.edu/project-svw.html) dataset is fed into the [Qwen 2.5 Vision-Language model](https://github.com/QwenLM/Qwen2.5-VL).\n",
    "- Here, the [Qwen2.5-VL-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct) model variant is used to process these videos and generate descriptions. The Qwen 2.5 Vision-Language model is loaded using the [PyTorch XPU backend](https://docs.pytorch.org/docs/stable/notes/get_start_xpu.html) to leverage Intel hardware acceleration.\n",
    "- Next, the generated video descriptions are converted into embeddings using [Sentence Transformers](https://sbert.net/), with the [all-MiniLM-L6-v2 model](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2).\n",
    "- These embeddings, along with the descriptions and video metadata, are stored in a persistent local [ChromaDB](https://www.trychroma.com/) collection. This is a one-time operation; since ChromaDB is local and persistent, it does not need to be repeated unless new videos are added.\n",
    "- When a user submits a query, the text is similarly encoded into an embedding, which is then used to perform a semantic search (via cosine similarity) over the ChromaDB collection.\n",
    "- The final result will be the most relevant video description and its associated video file name, and the video is displayed directly in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441de362-5dd5-4fd4-8847-0461adb0d006",
   "metadata": {},
   "source": [
    "![Video_description_generation_and_query_retrieval_workflow.jpg](./assets/Video_description_generation_and_query_retrieval_workflow.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214bbd89-9b6d-48e8-904d-d08a7450fae0",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d24b9f9f-c73b-4f91-91f9-cae38eb75e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import shutil\n",
    "import zipfile\n",
    "import logging\n",
    "import chromadb\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from IPython.display import Video, display\n",
    "from huggingface_hub import notebook_login\n",
    "from qwen_vl_utils import process_vision_info\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd9cf5b-3465-43d0-87bd-b1df7831d5c5",
   "metadata": {},
   "source": [
    "## Login to Huggingface to download the models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f306fbcb-82d7-482a-8b53-b1486f81d639",
   "metadata": {},
   "source": [
    "- Log in to [Huggingface](https://huggingface.co/) using your credentials.\n",
    "- You’ll need a [User Access Token](https://huggingface.co/docs/hub/security-tokens), which you can generate from your [Settings page](https://huggingface.co/settings/tokens). This token is used to authenticate your identity with the Hugging Face Hub.\n",
    "- Once you've generated the token, copy it and keep it secure. Then, run the cell below and paste your access token when prompted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36f2f1da-9918-412e-b6f1-41eaf41946ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64d1d0ce55f4b068d2cf4dc2b468713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7641924-264d-4d35-99a0-afa00d8b941b",
   "metadata": {},
   "source": [
    "## Extract ZIP file and get video file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd720d58-7b01-48ed-b166-717c6d6afb07",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def extract_video_paths():\n",
    "    \"\"\"\n",
    "    Extracts video files from the ZIP archive. Select the number of videos to select in the directory and return the video file paths.\n",
    "\n",
    "    Returns:\n",
    "        list: Selected list of paths to the extracted video files.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Raises an exception if there is any error during extracting video dataset.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        zip_path = \"SVW_subset_video_dataset.zip\"\n",
    "        dataset_folder = \"SVW_subset_video_dataset\"\n",
    "        if not os.path.exists(dataset_folder):\n",
    "            logging.info(\" Extracting video dataset..\")\n",
    "            with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "                zip_ref.extractall()\n",
    "            logging.info(\" Extraction complete.\")\n",
    "        else:\n",
    "            logging.info(\" Found video dataset directory\")\n",
    "        video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.flv', '.wmv']\n",
    "        video_files = []\n",
    "        for root, dirs, files in os.walk(dataset_folder):\n",
    "            video_files.extend([os.path.join(root, f) for f in files if any(f.lower().endswith(ext) for ext in video_extensions)])\n",
    "        total_video_files = len(video_files)\n",
    "        max_videos_to_select = 100\n",
    "        num_videos_to_select = min(total_video_files, max_videos_to_select)\n",
    "        random.seed(42)\n",
    "        selected_video_files = random.sample(video_files, num_videos_to_select)\n",
    "        logging.info(f\" Total number of video files found: {total_video_files}\\n\")\n",
    "        logging.info(f\" Selected {num_videos_to_select} video files:\")\n",
    "        for file in selected_video_files:\n",
    "            logging.info(file)\n",
    "        return selected_video_files\n",
    "    except Exception as e:\n",
    "        logging.exception(f\" Error while extracting the video paths: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b333d64-3ba6-477e-ac09-14ecfcf3833e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: Found video dataset directory\n",
      "INFO:root: Total number of video files found: 100\n",
      "\n",
      "INFO:root: Selected 100 video files:\n",
      "INFO:root:SVW_subset_video_dataset\\soccer\\252___64eaaaffa5284ed8aae4682f4b08e9ba.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\bowling\\170___2e2237fd3f1349c6aba27ab497659f27.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\baseball\\136___a5ae4b2b86274243b593a3194e2ff6f9.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\weight\\15___b3f6ea0174d144b0a4c1fcc7910ec8f5.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\golf\\10___b918ec5abe94452795b4f0f65637bd84.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\football\\47___9ad16112bc1e4a0c97b066135f03d1b4.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\diving\\192___6098c681c7f24973980e51e3b61e4429.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\bowling\\60___2417a63dd4b84e02ab8485ec3bf10bb3.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\bmx\\329___4ad9a8015a6044468fb06b4ae758eeda.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\tennis\\145___14d52737e37b4fbe8b4aadf4fecaa78a.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\running\\54___ed963024188f4f4486134848d1956137.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\bmx\\183___4fa2e7c4b2d0477abfde87856ef826fe.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\skating\\65___01b43ab8e8d944d1b1c2c197314c5697.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\hurdling\\738___fe4dc0b7124b45fbb6782475198eb8a4.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\baseball\\26___d5f47b2d16c84967b39eb5ef9f5544d2.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\wrestling\\632___151d0a1a2963444ab7a00a100a1e3e16.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\tennis\\400___ec7ac61453fa4f929743f35478679998.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\discusthrow\\830___fa2d41b5ca104749bf8c83f175f37866.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\diving\\217___db72bd8012dc4deba8361b8404881fd7.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\rowing\\14___68fa5a5497184829831241b86fb68cb5.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\skiing\\675___f7f5a5beab4d44e187710d144c21487e.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\swimming\\21___f257ea56e1124574a038c4dcdafe5d9b.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\shotput\\513___b52a9f4f9ce6464090506d26a867fc46.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\discusthrow\\353___33dbb6606d2c49e39e9c412c476c0a41.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\tennis\\629___de9685dd7c0d46769232051fb3a5d151.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\hurdling\\475___701f4379f9e24a5894976fab1922d34e.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\volleyball\\99___41058ebcf2d044d0b268f96779acfe94.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\javelin\\4___3e61b02bd5184494aa4690d0670dbb11.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\weight\\62___6f39d043fa7a41d780b129d9bceb66cc.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\archery\\235___ac6a72dc95e64a1da239285b073985b8.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\boxing\\571___6538518177a846e98dfc735961500455.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\volleyball\\106___a9aa810332d34e0ca94d29b235b894d6.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\hammerthrow\\547___35ee9e3b64b64971bfef9cfdcbe41c02.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\soccer\\168___22083b811c444f438c4e8dc464b28278.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\boxing\\1566___8c414ed34bf849868634a9eba5c06894.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\soccer\\45___bf93e68d23d34954ac19ba95db0c97e0.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\running\\106___0205f35e2fad42b1bd11ef4aefb22670.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\baseball\\5___10649b879e334cf9a517a0c6aeb85931.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\baseball\\33___7792b418db734dde9d238778a0f1f27c.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\cheerleading\\243___cf71bdaa7d2a4608923d7e2663f26f97.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\polevault\\44___363a88fc53834cea9c29cc59c8e26204.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\cheerleading\\110___23da3be84d284c97aefdf2e8d00053f0.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\running\\137___e6fffd316d8741eba910d7c4bb039ae4.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\longjump\\103___7a39f6ae79a648be9eacd68a65a96aa5.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\golf\\36___5651dabf094b41fdaa9b1d19f72645ff.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\bowling\\52___471780494c174b8b9e4d5e1b71eb2bba.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\hockey\\91___327f99c4bc8d4726b3ab4113b4d063d9.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\archery\\46___cf3e328ce61448308b9e9ff3517b4aad.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\highjump\\141___975a30d88a33428a9303a8cadb2376d7.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\wrestling\\78___5dd3c818e9e549e2a44a7850e515f101.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\football\\90___bd1f755ddd9147059595e990a46ee353.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\basketball\\162___2c6bfd4a66794f1b90f807d3b34e2d31.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\longjump\\32___1bede331140c42c8b0b46a2cfb9ffbe1.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\polevault\\17___bbd9593cb458417ab308ad1d8bb04ecb.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\rowing\\82___ad1d27601d704025aef5d15b0dd31dd6.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\boxing\\1455___430099f52dd04dcfaa01104d8fd6940d.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\gymnastics\\42___8cbef5afab524d9a91ba6999d7630d44.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\gymnastics\\18___f9f5efe4c3ae41b392ddc91300ae9a57.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\cheerleading\\237___58b78af4ac144e6388a20fc0fcb7a5f3.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\golf\\20___b98e0203cd784137bc7b9c1b2a47ae57.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\bmx\\291___b3c2ff4d2cdd43caa95e77b3db1faa22.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\swimming\\45___1d6b948ce987440bbd5a4f4332c442e1.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\hurdling\\295___7130e4846d92413d92c3fb200388dc0e.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\wrestling\\744___37752aeec1b24050acfc78ff864569d3.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\hammerthrow\\987___0c74d6762c394068962b7352ae533fb5.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\skating\\200___72711b13c6944d61b9b9d1f1fa17cd1d.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\polevault\\6___81a30a600a784637b67ceacf9bd8abce.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\longjump\\146___fc3547c70e7c4c4b9bbd4dfeff9369aa.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\highjump\\16___e88453acae28432e962a3059be9ea97e.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\basketball\\25___fd42a2fdba0d4c2fa72b6800ee0914d4.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\football\\89___fdeb3b446c5742ca94e7b94002adf1b6.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\tennis\\229___a9e0cbc099024f70b19625ae67c05f8f.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\discusthrow\\735___efa53cd6eff0462c92cd13ae78f3663f.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\swimming\\12___75eedbdebeb047d1b79396fa9bc77789.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\hockey\\166___70837b12ec39433a91d6bd547a216819.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\soccer\\175___5618d2991ac34351b871caf14ae4b4d4.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\weight\\67___849dd6800cc745338cd5ce328def2a1c.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\football\\61___7fbebddcdd27444a85e0a5df0ccf9564.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\boxing\\68___f3a86f4ec02841eea04b3cf7a9e40de2.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\diving\\334___f632cff42b2545f2ba9023e1aca7512e.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\golf\\27___005a928b047c4cd48374c17362ddf8e1.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\skiing\\488___908f3854c6974b9fb6af4ab4a3c4ee16.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\volleyball\\50___4d4579b5a56e48e5bba6acbbe98d07b8.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\hockey\\144___e74e37d5724c4445ba95e0cc70573fad.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\highjump\\102___0b361fecc6ff46218c69f263ab859cbe.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\shotput\\552___67b033cd35284b0cb3b283d16ef30d31.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\javelin\\20___2932ac6d65eb439eaaec362f17f6ac70.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\javelin\\153___9278e7841c8c45ab900717f3c1589b77.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\basketball\\51___8b4bb2cf2e8b4810a10c6568bb99e4e1.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\skating\\108___a4e0b1498e004756bd1e5b106dc62d35.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\skiing\\797___74378f48683c4f43ba3ef0c58fcab4b3.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\bowling\\325___4097661c7d0e4d80b085920e3fa0dfd9.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\shotput\\179___7ffe0656ce7c4a6b93ffd16fbf7c0eb3.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\archery\\298___dba56a1d93f84561927e5881edf0c670.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\hockey\\7___243cc23e4b2f47e1893f09863845b934.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\rowing\\306___293f5089adc54ab39c866226e68ea866.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\basketball\\49___9e25e3a078404687837d2c7b7ed34e45.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\volleyball\\2___59589caa29034f41b53e0aa19227a519.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\gymnastics\\58___bed1133eca984767a67a1e8e68160a47.mp4\n",
      "INFO:root:SVW_subset_video_dataset\\hammerthrow\\27___eedf5982ac514735a209e9b349bce298.mp4\n"
     ]
    }
   ],
   "source": [
    "selected_video_files = extract_video_paths()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bbdd4d-b39f-4f01-bd9d-87c925eec9e5",
   "metadata": {},
   "source": [
    "## Initialize models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1a34422-4b88-4ef9-ba87-9e4a88d87b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_models():\n",
    "    \"\"\"\n",
    "    Initialize Vision Language and Sentence Transformer models.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containg the initialized Vision Language and Sentence Transformer models.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Raises an exception if the model loading fails.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model_id = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "        embedding_model_id = \"all-MiniLM-L6-v2\"\n",
    "        vl_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(pretrained_model_name_or_path=model_id,\n",
    "                                                                      torch_dtype=torch.float16)\n",
    "        processor = AutoProcessor.from_pretrained(pretrained_model_name_or_path=model_id,\n",
    "                                                  min_pixels=256*14*14,\n",
    "                                                  max_pixels=768*28*28,\n",
    "                                                  use_fast=True)\n",
    "        device = \"xpu\" if torch.xpu.is_available() else \"cpu\"\n",
    "        vl_model = vl_model.to(device)\n",
    "        logging.info(f\" Loading Qwen Vision Language Model: {model_id}\")\n",
    "        logging.info(f\" Loading Qwen Vision Language Model onto the Pytorch device: {device}\")\n",
    "        embedding_model = SentenceTransformer(embedding_model_id)        \n",
    "        return {\n",
    "            \"vl_model\": vl_model,\n",
    "            \"processor\": processor,\n",
    "            \"embedding_model\": embedding_model\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logging.exception(f\" Error while loading the models: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8eafdcf3-dc6a-4aa0-90a2-e6d41a552f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa15c58439a7453ead8b200e8f9c11c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: Loading Qwen Vision Language Model: Qwen/Qwen2.5-VL-3B-Instruct\n",
      "INFO:root: Loading Qwen Vision Language Model onto the Pytorch device: xpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Use pytorch device_name: cpu\n",
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "models = initialize_models()\n",
    "vl_model = models[\"vl_model\"]\n",
    "processor = models[\"processor\"]\n",
    "embedding_model = models[\"embedding_model\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3d3cce-ac20-4ccd-ba99-50d11ed295bb",
   "metadata": {},
   "source": [
    "## Get or create Chromadb collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f3cf650-fd55-435e-babb-28d96974ab01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_or_create_database():\n",
    "    \"\"\"\n",
    "    Connects to or creates a persistant ChromaDB collection for storing the Video descriptions and filenames.\n",
    "\n",
    "    Returns:\n",
    "        collection: The ChromaDB collection object.\n",
    "        existing_descriptions (dict): The dictionary contains video descriptions of the existing Chromadb database collection.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Raises an exception if there is any error while checking an existing database.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        client = chromadb.PersistentClient(path=\"Video_descriptions_database\")\n",
    "        collection = client.get_or_create_collection(\n",
    "            name=\"Video_descriptions\", \n",
    "            configuration={\n",
    "                \"hnsw\": {\n",
    "                    \"space\": \"cosine\",\n",
    "                },\n",
    "            }\n",
    "        )\n",
    "        logging.info(\" Checking existing descriptions in database..\")\n",
    "        all_items = collection.get(include=[\"metadatas\", \"documents\"])\n",
    "        existing_descriptions = {}\n",
    "        for metadata, doc in zip(all_items['metadatas'], all_items['documents']):\n",
    "            existing_descriptions[metadata['video_filename']] = doc\n",
    "        logging.info(f\" Found {len(existing_descriptions)} existing descriptions\")\n",
    "        return collection, existing_descriptions\n",
    "    except Exception as e:\n",
    "        existing_descriptions = {}\n",
    "        logging.exception(\" No existing descriptions found\")\n",
    "        logging.exception(f\" Error while checking an existing database: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "940150ef-4bea-4a85-9b06-46df884197b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:chromadb.telemetry.product.posthog:Anonymized telemetry enabled. See                     https://docs.trychroma.com/telemetry for more information.\n",
      "INFO:root: Checking existing descriptions in database..\n",
      "INFO:root: Found 100 existing descriptions\n"
     ]
    }
   ],
   "source": [
    "collection, existing_descriptions = get_or_create_database()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc99ee8b-0b84-4ca8-8e7c-3428493386c3",
   "metadata": {},
   "source": [
    "## Generate and store video descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec42261b-8d9e-4173-be4c-9f347ff19a67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_and_store_video_descriptions(selected_video_files, collection, existing_descriptions, vl_model, processor, embedding_model):\n",
    "    \"\"\"\n",
    "    Check and generate video description for each video file in the selected directory.\n",
    "    Store the embeddings of generated video descriptions in the ChromaDB collection.\n",
    "    \n",
    "    Args:\n",
    "        selected_video_files (list): List of selected video files.\n",
    "        collection: The ChromaDB collection object.\n",
    "        existing_descriptions (dict): The dictionary contains video descriptions of the existing Chromadb database collection.\n",
    "        vl_model: Vision Language Model for generating video descriptions.\n",
    "        processor: Vision Language Model processor for processing videos.\n",
    "        embedding_model: Sentence Transformer Model for generating embeddings.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Raises an exception if there is any error while generating video descriptions or storing the embeddings in the database.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        video_descriptions = {}\n",
    "        for i, video_file in enumerate(tqdm(selected_video_files)):\n",
    "            video_filename = os.path.basename(video_file)\n",
    "            if video_filename in existing_descriptions:\n",
    "                logging.info(f\" Skipping file {os.path.basename(video_file)} - video description already present in the database\")\n",
    "                video_descriptions[video_file] = existing_descriptions[video_filename]\n",
    "                continue\n",
    "\n",
    "            video_path = video_file\n",
    "            torch.xpu.empty_cache()\n",
    "            logging.info(f\"\\n Processing file {os.path.basename(video_file)} using Qwen 2.5 VL on Pytorch XPU..\")\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"video\",\n",
    "                            \"video\": video_path,\n",
    "                            \"max_pixels\": 320*240,\n",
    "                            \"fps\": 0.5,\n",
    "                        },\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": \"Describe this sports video.\"\n",
    "                        },\n",
    "                    ],\n",
    "                }\n",
    "            ]\n",
    "            text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "            image_inputs, video_inputs = process_vision_info(messages)\n",
    "            inputs = processor(\n",
    "                text=[text],\n",
    "                images=image_inputs,\n",
    "                videos=video_inputs,\n",
    "                padding=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            inputs = inputs.to(\"xpu\")\n",
    "            generated_ids = vl_model.generate(**inputs, max_new_tokens=128)\n",
    "            generated_ids_trimmed = [\n",
    "                out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "            ]\n",
    "            output_text = processor.batch_decode(\n",
    "                generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "            )\n",
    "            description_text = output_text[0] if isinstance(output_text, list) else str(output_text)\n",
    "            video_descriptions[video_file] = description_text\n",
    "            logging.info(f\" Generated description: {description_text}.\\n\")\n",
    "            # video_descriptions[video_file] = output_text\n",
    "            embedding = embedding_model.encode(sentences=description_text).tolist()\n",
    "            collection.add(\n",
    "                embeddings=[embedding],\n",
    "                documents=[description_text],\n",
    "                metadatas=[{\"video_filename\": os.path.basename(video_file)}],\n",
    "                ids=[video_file]\n",
    "            )\n",
    "            logging.info(f\" Added {os.path.basename(video_file)} video file description to database\\n\\n ******\")\n",
    "        logging.info(f\" Processed {len(video_descriptions)} videos\")\n",
    "        logging.info(f\" Database now has {collection.count()} total descriptions\")\n",
    "    except Exception as e:\n",
    "        logging.exception(f\" Error while generating and storing video descriptions: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "829fedae-16d5-47f8-8d2a-447e78d3e3bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'selected_video_files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m generate_and_store_video_descriptions(\u001b[43mselected_video_files\u001b[49m, collection, existing_descriptions, vl_model, processor, embedding_model)    \n",
      "\u001b[31mNameError\u001b[39m: name 'selected_video_files' is not defined"
     ]
    }
   ],
   "source": [
    "generate_and_store_video_descriptions(selected_video_files, collection, existing_descriptions, vl_model, processor, embedding_model)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb31cd1f-af0f-41bd-98b8-b11115f60699",
   "metadata": {},
   "source": [
    "## Query the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "926ce37e-2272-489b-be0f-719dc109f385",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_videos_descriptions(query, collection):\n",
    "    \"\"\"\n",
    "    Queries the ChromaDB collection to find the similar video relevant to the input query.\n",
    "\n",
    "    Args:\n",
    "        query (str): User query.\n",
    "        collection: The ChromaDB collection object.\n",
    "\n",
    "    Returns:\n",
    "        results (dict): Query results containing video descriptions and video file name.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Raises an exception if there is any error while querying the database.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        query_embedding = embedding_model.encode(query).tolist()\n",
    "        results = collection.query(\n",
    "            query_embeddings=[query_embedding],\n",
    "            n_results=1,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\", \"embeddings\"]\n",
    "        )\n",
    "        logging.info(f\" Search results for: '{query}'\\n\")\n",
    "        for i, (doc, metadata, distance) in enumerate(zip(\n",
    "            results['documents'][0],\n",
    "            results['metadatas'][0],\n",
    "            results['distances'][0]\n",
    "        )):\n",
    "            similarity_score = 1 - distance\n",
    "            logging.info(f\" Video filename: {metadata['video_filename']} (Similarity score: {similarity_score:.3f})\\n\")\n",
    "            logging.info(f\" Distance: {distance:.3f}\\n\")\n",
    "            logging.info(f\" Video description: {doc}\\n\")\n",
    "        return results\n",
    "    except Exception as e:\n",
    "        logging.exception(f\" Error while querying video descriptions: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4215e3e8-7ad4-4690-bc4b-1b852ad217b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1e25e4d79743868d106463a2016a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root: Search results for: 'girl practicing in a golf club'\n",
      "\n",
      "INFO:root: Video filename: 10___b918ec5abe94452795b4f0f65637bd84.mp4 (Similarity score: 0.720)\n",
      "\n",
      "INFO:root: Distance: 0.280\n",
      "\n",
      "INFO:root: Video description: The video shows a young girl practicing her golf swing on a grassy course. She is holding a golf club and appears to be hitting golf balls. The background features other people, likely fellow golfers or spectators, and the landscape includes mountains in the distance. The setting suggests a sunny day with clear skies.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"girl practicing in a golf club\"\n",
    "results = query_videos_descriptions(query, collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e356e619-9342-48d6-b680-5346f4bfbbe0",
   "metadata": {},
   "source": [
    "## Display the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c657bfcd-6fce-40ee-a9b4-bf3c4f0de3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_video(results):\n",
    "    \"\"\"\n",
    "    Display the video based the query results.\n",
    "\n",
    "    Args:\n",
    "        results (dict): Query results containing video descriptions and video file name.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Raises an exception if there is any error while displaying the video.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        video = Video(results['ids'][0][0], width=600, height=400)\n",
    "        display(video)\n",
    "    except Exception as e:\n",
    "        logging.exception(f\"Error while displaying the video: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eaf0af90-0eb1-423b-8d0f-3f67c2cc877a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video src=\"SVW_subset_video_dataset\\golf\\10___b918ec5abe94452795b4f0f65637bd84.mp4\" controls  width=\"600\"  height=\"400\">\n",
       "      Your browser does not support the <code>video</code> element.\n",
       "    </video>"
      ],
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_video(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55db9d3-b2e7-4826-8c1b-a1acb288ec9d",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Remove the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9a08316-779e-4c35-bb83-242f6a9e0a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_database():\n",
    "    \"\"\"\n",
    "    Deleted the database directory.\n",
    "\n",
    "    Raises:\n",
    "        Exception: Raises an exception if there is any error while deleting the database.\n",
    "    \"\"\"\n",
    "    database_folder = \"Video_descriptions_database\"\n",
    "    if os.path.exists(database_folder):\n",
    "        logging.info(\"You are about to delete the database. Once deleted, it will no longer be available, and you will need to regenerate and store the video descriptions again.\")\n",
    "        answer = input(f\"Do you want to remove the database '{database_folder}'? \\nType 'yes' to remove: \")\n",
    "        if answer == 'yes':\n",
    "            try:\n",
    "                shutil.rmtree(database_folder)\n",
    "                logging.info(\"Database deleted!\")\n",
    "            except Exception as e:\n",
    "                logging.exception(f\"Error while deleting the database: {str(e)}\")\n",
    "        else:\n",
    "            logging.info(\"Database not deleted\")\n",
    "    else:\n",
    "        logging.info(\"Database is not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47d4486c-9165-4ebe-85be-7ed036634624",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Database is not available\n"
     ]
    }
   ],
   "source": [
    "delete_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f511c5b-3962-4fc2-81df-bb02f3302690",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe8a1e8e-aefa-40fb-bc31-18320c6b21f5",
   "metadata": {},
   "source": [
    "## Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdcad7c1-3b91-4a07-be0b-770975fad53e",
   "metadata": {},
   "source": [
    "If you use SVW dataset, please refer to this paper in your publications:\n",
    "\n",
    "### Publications\n",
    "- [**Sports Videos in the Wild (SVW): A Video Dataset for Sports Analysis**](https://cvlab.cse.msu.edu/project-svw.html)\\\n",
    "[**Seyed Morteza Safdarnejad**](https://cvlab.cse.msu.edu/author/seyed-morteza-safdarnejad.html), [**Xiaoming Liu**](https://cvlab.cse.msu.edu/author/xiaoming-liu.html), [**Lalita Udpa**](https://cvlab.cse.msu.edu/author/lalita-udpa.html), [**Brooks Andrus**](https://cvlab.cse.msu.edu/author/brooks-andrus.html), [**John Wood**](https://cvlab.cse.msu.edu/author/john-wood.html), [**Dean Craven**](https://cvlab.cse.msu.edu/author/dean-craven.html)\\\n",
    "Proc. International Conference on Automatic Face and Gesture Recognition (FG 2015), Ljubljana, Slovenia, May. 2015 (Acceptance rate 84/221 = 38%)\\\n",
    "[**Bibtex**](https://cvlab.cse.msu.edu/project-svw.html#bibtex-sports-videos-in-the-wild-svw-a-video-dataset-for-sports-analysis) | [**PDF**](https://cvlab.cse.msu.edu/pdfs/Morteza_FG2015.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b987aec-dfe2-4eb7-8af5-5834cdcb0f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2130913d-950e-4c02-ac94-84933ee8290b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63234ac2-2053-464f-9d7a-e060e608f6c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MM_venv",
   "language": "python",
   "name": "mm_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
